{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joyparikh/ELEN6885_Project/blob/main/dqn_tf-agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This must be run within a Google Colab environment \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "SPgJG0WApelH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851c20cc-eb25-4934-91f1-8032e6ac0a01"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# sys.path.append('/content/gdrive/My Drive/RL/.')\n",
        "sys.path.append('/content/gdrive/My Drive/Rubiks')"
      ],
      "metadata": {
        "id": "JMm-volmpv1Y"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo apt-get update\n",
        "# !sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "# !pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install tf-agents[reverb]\n",
        "# !pip install pyglet"
      ],
      "metadata": {
        "id": "cQBT_aehqAUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc849e0-ec29-4831-ba15-0b332509787d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (2.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.3.0)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.13.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (7.1.2)\n",
            "Requirement already satisfied: tensorflow-probability>=0.14.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.15.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (3.17.3)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.12.0)\n",
            "Requirement already satisfied: tensorflow~=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (2.7.0)\n",
            "Requirement already satisfied: dm-reverb~=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents[reverb]) (0.6.1)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.6.0->tf-agents[reverb]) (1.3.9)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from dm-reverb~=0.6.0->tf-agents[reverb]) (0.1.6)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents[reverb]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents[reverb]) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents[reverb]) (0.16.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (2.7.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (3.1.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (1.42.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (12.0.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (2.7.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (0.37.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.7.0->tf-agents[reverb]) (0.22.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow~=2.7.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (1.8.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->tf-agents[reverb]) (3.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.14.1->tf-agents[reverb]) (4.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "fPQVLS3lpc6u"
      },
      "outputs": [],
      "source": [
        "# from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "# import pyvirtualdisplay\n",
        "import reverb\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "CU1ST-ocpc6-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment \n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.policies import PolicySaver\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "abTlwIJhpc6_"
      },
      "outputs": [],
      "source": [
        "num_iterations = 24000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration =  50  # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "# learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "\n",
        "boundaries = [50000, 100000]\n",
        "values = [1e-3, (1e-3)/3, 1e-4]\n",
        "learning_rate = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries, values)\n",
        "\n",
        "log_interval = 500  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 2000  # @param {type:\"integer\"}\n",
        "\n",
        "discount = 0.98"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "82bnFcMfpc7B"
      },
      "outputs": [],
      "source": [
        "import gym.spaces\n",
        "import gym_Rubiks_Cube\n",
        "import gym\n",
        "\n",
        "env_name = \"RubiksCube-v0\"\n",
        "gym.make(env_name)\n",
        "\n",
        "env = suite_gym.load(env_name)\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "OBG5ViChpc7D"
      },
      "outputs": [],
      "source": [
        "fc_layer_params = (1000,1000, 1000, 300)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=0.002, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.00003, maxval=0.00003),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.00002))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "UyhYLwf-pc7H"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter,\n",
        "    gamma = discount)\n",
        "\n",
        "agent.initialize()\n",
        "\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "p1HAYsSdpc7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f46a33e0-80eb-4905-f8a5-3bbecbdc2adc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fbc3bfec610>\n"
          ]
        }
      ],
      "source": [
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)\n",
        "\n",
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_py_env.reset())\n",
        "\n",
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset\n",
        "\n",
        "iterator = iter(dataset)\n",
        "print(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def two_move(policy):\n",
        "  environment = suite_gym.load(env_name)\n",
        "  environment.setScramble(1, 1, False)\n",
        "  environment.setStepMax(4)\n",
        "  environment = tf_py_environment.TFPyEnvironment(environment)\n",
        "\n",
        "  solved = []\n",
        "\n",
        "  for action in range(12):\n",
        "    environment.reset()\n",
        "    time_step = environment.step(action)\n",
        "  \n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      print(action, action_step.action)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "\n",
        "    if episode_return >= 1:\n",
        "      solved.append(action)\n",
        "  return solved\n",
        "\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]"
      ],
      "metadata": {
        "id": "1JBWwiQzTBCA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "returns = {i:[0,0] for i in range(9)}\n",
        "\n",
        "for i in range(1, 9):\n",
        "  # Reset the environment.\n",
        "  time_step = train_py_env.reset()\n",
        "\n",
        "  env = suite_gym.load(env_name)\n",
        "  env.setScramble(1, i)\n",
        "  env.setStepMax(3*i + 1)\n",
        "  eval_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "  # Create a driver to collect experience.\n",
        "  collect_driver = py_driver.PyDriver(\n",
        "      env,\n",
        "      py_tf_eager_policy.PyTFEagerPolicy(\n",
        "        agent.collect_policy, use_tf_function=True),\n",
        "      [rb_observer],\n",
        "      max_steps=collect_steps_per_iteration)\n",
        "\n",
        "  while returns[i][-2] < 0.93 or returns[i][-1] < 0.93:\n",
        "\n",
        "    # Collect a few steps and save to the replay buffer.\n",
        "    time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "    # Sample a batch of data from the buffer and update the agent's network.\n",
        "    experience, unused_info = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "\n",
        "    step = agent.train_step_counter.numpy()\n",
        "\n",
        "    if step % log_interval == 0:\n",
        "      print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "    if step % eval_interval == 0:\n",
        "      avg_return = compute_avg_return(eval_env, agent.policy, i*100)\n",
        "      print('scramble = {0}: step = {1}: Average Return = {2}'.format(i, step, avg_return))\n",
        "      returns[i].append(avg_return)\n",
        "\n",
        "      if i == 1:\n",
        "        solutions = two_move(agent.policy)\n",
        "        print('step = {0}: Solutions = {1}'.format(step, solutions))\n",
        "      \n",
        "      if returns[i][-2] > 0.93 and avg_return > 0.93:\n",
        "        saver = PolicySaver(agent.collect_policy, batch_size=None)\n",
        "        saver.save(os.path.join('/content/gdrive/My Drive/Rubiks',f'policy_{i}'))\n",
        "        break\n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "uDe5TD38IEmg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe5e253f-c70a-4e12-ddd2-6724f7273b4c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 500: loss = 0.06076683849096298\n",
            "step = 1000: loss = 0.03781650960445404\n",
            "step = 1500: loss = 0.05192701518535614\n",
            "step = 2000: loss = 0.04798615723848343\n",
            "scramble = 1: step = 2000: Average Return = 0.44999998807907104\n",
            "0 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "0 tf.Tensor([10], shape=(1,), dtype=int64)\n",
            "0 tf.Tensor([10], shape=(1,), dtype=int64)\n",
            "0 tf.Tensor([10], shape=(1,), dtype=int64)\n",
            "1 tf.Tensor([10], shape=(1,), dtype=int64)\n",
            "1 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "1 tf.Tensor([10], shape=(1,), dtype=int64)\n",
            "1 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "3 tf.Tensor([9], shape=(1,), dtype=int64)\n",
            "4 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "4 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "4 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "6 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "7 tf.Tensor([10], shape=(1,), dtype=int64)\n",
            "7 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "7 tf.Tensor([10], shape=(1,), dtype=int64)\n",
            "7 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "8 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "8 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "8 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "8 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "9 tf.Tensor([9], shape=(1,), dtype=int64)\n",
            "9 tf.Tensor([9], shape=(1,), dtype=int64)\n",
            "9 tf.Tensor([9], shape=(1,), dtype=int64)\n",
            "10 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "11 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "11 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "11 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "11 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "step = 2000: Solutions = [3, 4, 6, 9, 10]\n",
            "step = 2500: loss = 0.04519239813089371\n",
            "step = 3000: loss = 0.026894615963101387\n",
            "step = 3500: loss = 0.026653187349438667\n",
            "step = 4000: loss = 0.007625117432326078\n",
            "scramble = 1: step = 4000: Average Return = 0.800000011920929\n",
            "0 tf.Tensor([6], shape=(1,), dtype=int64)\n",
            "1 tf.Tensor([7], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([3], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([3], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([3], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([3], shape=(1,), dtype=int64)\n",
            "3 tf.Tensor([9], shape=(1,), dtype=int64)\n",
            "4 tf.Tensor([10], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([5], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([6], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([5], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([5], shape=(1,), dtype=int64)\n",
            "6 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "7 tf.Tensor([1], shape=(1,), dtype=int64)\n",
            "8 tf.Tensor([5], shape=(1,), dtype=int64)\n",
            "8 tf.Tensor([6], shape=(1,), dtype=int64)\n",
            "8 tf.Tensor([6], shape=(1,), dtype=int64)\n",
            "8 tf.Tensor([6], shape=(1,), dtype=int64)\n",
            "9 tf.Tensor([9], shape=(1,), dtype=int64)\n",
            "9 tf.Tensor([9], shape=(1,), dtype=int64)\n",
            "9 tf.Tensor([9], shape=(1,), dtype=int64)\n",
            "10 tf.Tensor([10], shape=(1,), dtype=int64)\n",
            "10 tf.Tensor([10], shape=(1,), dtype=int64)\n",
            "10 tf.Tensor([10], shape=(1,), dtype=int64)\n",
            "11 tf.Tensor([5], shape=(1,), dtype=int64)\n",
            "step = 4000: Solutions = [0, 1, 3, 4, 6, 7, 9, 10, 11]\n",
            "step = 4500: loss = 0.049278125166893005\n",
            "step = 5000: loss = 0.03068651258945465\n",
            "step = 5500: loss = 0.045415617525577545\n",
            "step = 6000: loss = 0.027154531329870224\n",
            "scramble = 1: step = 6000: Average Return = 0.949999988079071\n",
            "0 tf.Tensor([6], shape=(1,), dtype=int64)\n",
            "1 tf.Tensor([7], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([2], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([2], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([2], shape=(1,), dtype=int64)\n",
            "3 tf.Tensor([3], shape=(1,), dtype=int64)\n",
            "3 tf.Tensor([3], shape=(1,), dtype=int64)\n",
            "3 tf.Tensor([3], shape=(1,), dtype=int64)\n",
            "4 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "4 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "4 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([6], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([6], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([5], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([2], shape=(1,), dtype=int64)\n",
            "6 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "7 tf.Tensor([1], shape=(1,), dtype=int64)\n",
            "8 tf.Tensor([2], shape=(1,), dtype=int64)\n",
            "9 tf.Tensor([3], shape=(1,), dtype=int64)\n",
            "10 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "11 tf.Tensor([5], shape=(1,), dtype=int64)\n",
            "step = 6000: Solutions = [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11]\n",
            "step = 6500: loss = 0.051344506442546844\n",
            "step = 7000: loss = 0.03121795505285263\n",
            "step = 7500: loss = 0.019540216773748398\n",
            "step = 8000: loss = 0.012929560616612434\n",
            "scramble = 1: step = 8000: Average Return = 0.9399999976158142\n",
            "0 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "0 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "0 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "1 tf.Tensor([7], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([2], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([2], shape=(1,), dtype=int64)\n",
            "2 tf.Tensor([2], shape=(1,), dtype=int64)\n",
            "3 tf.Tensor([9], shape=(1,), dtype=int64)\n",
            "4 tf.Tensor([10], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([5], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([5], shape=(1,), dtype=int64)\n",
            "5 tf.Tensor([5], shape=(1,), dtype=int64)\n",
            "6 tf.Tensor([0], shape=(1,), dtype=int64)\n",
            "7 tf.Tensor([7], shape=(1,), dtype=int64)\n",
            "7 tf.Tensor([1], shape=(1,), dtype=int64)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7fbb020e9c10>\". Calling saved_model.distribution() will raise the following assertion error: EpsilonGreedyPolicy does not support distributions yet.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7 tf.Tensor([7], shape=(1,), dtype=int64)\n",
            "7 tf.Tensor([1], shape=(1,), dtype=int64)\n",
            "8 tf.Tensor([2], shape=(1,), dtype=int64)\n",
            "9 tf.Tensor([3], shape=(1,), dtype=int64)\n",
            "10 tf.Tensor([4], shape=(1,), dtype=int64)\n",
            "11 tf.Tensor([5], shape=(1,), dtype=int64)\n",
            "step = 8000: Solutions = [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7fbb020e9c10>\". Calling saved_model.distribution() will raise the following assertion error: EpsilonGreedyPolicy does not support distributions yet.\n",
            "WARNING:absl:Found untraced functions such as sequential_2_layer_call_fn, sequential_2_layer_call_and_return_conditional_losses, sequential_2_layer_call_fn, sequential_2_layer_call_and_return_conditional_losses, sequential_2_layer_call_and_return_conditional_losses while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Rubiks/policy_1/assets\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Rubiks/policy_1/assets\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 8500: loss = 0.010713191702961922\n",
            "step = 9000: loss = 0.009588990360498428\n",
            "step = 9500: loss = 0.0162886381149292\n",
            "step = 10000: loss = 0.027351953089237213\n",
            "scramble = 2: step = 10000: Average Return = 0.824999988079071\n",
            "step = 10500: loss = 0.027921758592128754\n",
            "step = 11000: loss = 0.02967274934053421\n",
            "step = 11500: loss = 0.034893862903118134\n",
            "step = 12000: loss = 0.03953687846660614\n",
            "scramble = 2: step = 12000: Average Return = 0.8349999785423279\n",
            "step = 12500: loss = 0.02025412768125534\n",
            "step = 13000: loss = 0.016409965232014656\n",
            "step = 13500: loss = 0.033575497567653656\n",
            "step = 14000: loss = 0.041828546673059464\n",
            "scramble = 2: step = 14000: Average Return = 0.875\n",
            "step = 14500: loss = 0.0159151554107666\n",
            "step = 15000: loss = 0.007495248690247536\n",
            "step = 15500: loss = 0.010179983451962471\n",
            "step = 16000: loss = 0.02602536603808403\n",
            "scramble = 2: step = 16000: Average Return = 0.9399999976158142\n",
            "step = 16500: loss = 0.036865442991256714\n",
            "step = 17000: loss = 0.013967138715088367\n",
            "step = 17500: loss = 0.01045026071369648\n",
            "step = 18000: loss = 0.006686392240226269\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7fbb020e9c10>\". Calling saved_model.distribution() will raise the following assertion error: EpsilonGreedyPolicy does not support distributions yet.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scramble = 2: step = 18000: Average Return = 0.9399999976158142\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7fbb020e9c10>\". Calling saved_model.distribution() will raise the following assertion error: EpsilonGreedyPolicy does not support distributions yet.\n",
            "WARNING:absl:Found untraced functions such as sequential_2_layer_call_fn, sequential_2_layer_call_and_return_conditional_losses, sequential_2_layer_call_fn, sequential_2_layer_call_and_return_conditional_losses, sequential_2_layer_call_and_return_conditional_losses while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Rubiks/policy_2/assets\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Rubiks/policy_2/assets\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 18500: loss = 0.011672209948301315\n",
            "step = 19000: loss = 0.017299098894000053\n",
            "step = 19500: loss = 0.020323317497968674\n",
            "step = 20000: loss = 0.03703823685646057\n",
            "scramble = 3: step = 20000: Average Return = 0.4566666781902313\n",
            "step = 20500: loss = 0.021304139867424965\n",
            "step = 21000: loss = 0.013763321563601494\n",
            "step = 21500: loss = 0.015739407390356064\n",
            "step = 22000: loss = 0.027103040367364883\n",
            "scramble = 3: step = 22000: Average Return = 0.8799999952316284\n",
            "step = 22500: loss = 0.016139136627316475\n",
            "step = 23000: loss = 0.012004636228084564\n",
            "step = 23500: loss = 0.019659975543618202\n",
            "step = 24000: loss = 0.02396976947784424\n",
            "scramble = 3: step = 24000: Average Return = 0.9333333373069763\n",
            "step = 24500: loss = 0.02618854120373726\n",
            "step = 25000: loss = 0.03434820473194122\n",
            "step = 25500: loss = 0.012148233130574226\n",
            "step = 26000: loss = 0.011418061330914497\n",
            "scramble = 3: step = 26000: Average Return = 0.8666666746139526\n",
            "step = 26500: loss = 0.02549433335661888\n",
            "step = 27000: loss = 0.006964586675167084\n",
            "step = 27500: loss = 0.03418043255805969\n",
            "step = 28000: loss = 0.01875179260969162\n",
            "scramble = 3: step = 28000: Average Return = 0.9433333277702332\n",
            "step = 28500: loss = 0.031809668987989426\n",
            "step = 29000: loss = 0.010014340281486511\n",
            "step = 29500: loss = 0.024515587836503983\n",
            "step = 30000: loss = 0.0257857795804739\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7fbb020e9c10>\". Calling saved_model.distribution() will raise the following assertion error: EpsilonGreedyPolicy does not support distributions yet.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scramble = 3: step = 30000: Average Return = 0.95333331823349\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7fbb020e9c10>\". Calling saved_model.distribution() will raise the following assertion error: EpsilonGreedyPolicy does not support distributions yet.\n",
            "WARNING:absl:Found untraced functions such as sequential_2_layer_call_fn, sequential_2_layer_call_and_return_conditional_losses, sequential_2_layer_call_fn, sequential_2_layer_call_and_return_conditional_losses, sequential_2_layer_call_and_return_conditional_losses while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Rubiks/policy_3/assets\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Rubiks/policy_3/assets\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step = 30500: loss = 0.03865417093038559\n",
            "step = 31000: loss = 0.010133224539458752\n",
            "step = 31500: loss = 0.008871367201209068\n",
            "step = 32000: loss = 0.0345495231449604\n",
            "scramble = 4: step = 32000: Average Return = 0.8224999904632568\n",
            "step = 32500: loss = 0.01809445396065712\n",
            "step = 33000: loss = 0.034994952380657196\n",
            "step = 33500: loss = 0.018082264810800552\n",
            "step = 34000: loss = 0.0180258397012949\n",
            "scramble = 4: step = 34000: Average Return = 0.8700000047683716\n",
            "step = 34500: loss = 0.04020567238330841\n",
            "step = 35000: loss = 0.02343600243330002\n",
            "step = 35500: loss = 0.020180966705083847\n",
            "step = 36000: loss = 0.020971914753317833\n",
            "scramble = 4: step = 36000: Average Return = 0.875\n",
            "step = 36500: loss = 0.013558254577219486\n",
            "step = 37000: loss = 0.01824658177793026\n",
            "step = 37500: loss = 0.03399565443396568\n",
            "step = 38000: loss = 0.02891034260392189\n",
            "scramble = 4: step = 38000: Average Return = 0.8274999856948853\n",
            "step = 38500: loss = 0.013666462153196335\n",
            "step = 39000: loss = 0.01227203942835331\n",
            "step = 39500: loss = 0.0222848542034626\n",
            "step = 40000: loss = 0.015724707394838333\n",
            "scramble = 4: step = 40000: Average Return = 0.887499988079071\n",
            "step = 40500: loss = 0.018829941749572754\n",
            "step = 41000: loss = 0.03630148619413376\n",
            "step = 41500: loss = 0.03050052747130394\n",
            "step = 42000: loss = 0.0236215777695179\n",
            "scramble = 4: step = 42000: Average Return = 0.8949999809265137\n",
            "step = 42500: loss = 0.032877370715141296\n",
            "step = 43000: loss = 0.03364128991961479\n",
            "step = 43500: loss = 0.05195652320981026\n",
            "step = 44000: loss = 0.038891490548849106\n",
            "scramble = 4: step = 44000: Average Return = 0.8799999952316284\n",
            "step = 44500: loss = 0.017627548426389694\n",
            "step = 45000: loss = 0.012771431356668472\n",
            "step = 45500: loss = 0.004462208598852158\n",
            "step = 46000: loss = 0.032344840466976166\n",
            "scramble = 4: step = 46000: Average Return = 0.8824999928474426\n",
            "step = 46500: loss = 0.02644634246826172\n",
            "step = 47000: loss = 0.034134622663259506\n",
            "step = 47500: loss = 0.013577282428741455\n",
            "step = 48000: loss = 0.014869341626763344\n",
            "scramble = 4: step = 48000: Average Return = 0.875\n",
            "step = 48500: loss = 0.006552703212946653\n",
            "step = 49000: loss = 0.04370935261249542\n",
            "step = 49500: loss = 0.017412906512618065\n",
            "step = 50000: loss = 0.013661136850714684\n",
            "scramble = 4: step = 50000: Average Return = 0.9024999737739563\n",
            "step = 50500: loss = 0.007969306781888008\n",
            "step = 51000: loss = 0.011028468608856201\n",
            "step = 51500: loss = 0.006841610185801983\n",
            "step = 52000: loss = 0.012084742076694965\n",
            "scramble = 4: step = 52000: Average Return = 0.8924999833106995\n",
            "step = 52500: loss = 0.019297510385513306\n",
            "step = 53000: loss = 0.022528918460011482\n",
            "step = 53500: loss = 0.014360103756189346\n",
            "step = 54000: loss = 0.022228391841053963\n",
            "scramble = 4: step = 54000: Average Return = 0.9325000047683716\n",
            "step = 54500: loss = 0.021944893524050713\n",
            "step = 55000: loss = 0.015116030350327492\n",
            "step = 55500: loss = 0.007571350783109665\n",
            "step = 56000: loss = 0.013427656143903732\n",
            "scramble = 4: step = 56000: Average Return = 0.9125000238418579\n",
            "step = 56500: loss = 0.044423218816518784\n",
            "step = 57000: loss = 0.009658564813435078\n",
            "step = 57500: loss = 0.020164083689451218\n",
            "step = 58000: loss = 0.006584620103240013\n",
            "scramble = 4: step = 58000: Average Return = 0.925000011920929\n",
            "step = 58500: loss = 0.013442904688417912\n",
            "step = 59000: loss = 0.009918807074427605\n",
            "step = 59500: loss = 0.012457476928830147\n",
            "step = 60000: loss = 0.010501806624233723\n",
            "scramble = 4: step = 60000: Average Return = 0.9449999928474426\n",
            "step = 60500: loss = 0.011222859844565392\n",
            "step = 61000: loss = 0.03408130258321762\n",
            "step = 61500: loss = 0.009049064479768276\n",
            "step = 62000: loss = 0.03228490799665451\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7fbb020e9c10>\". Calling saved_model.distribution() will raise the following assertion error: EpsilonGreedyPolicy does not support distributions yet.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scramble = 4: step = 62000: Average Return = 0.9350000023841858\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Function `function_with_signature` contains input name(s) 0/step_type, 0/reward, 0/discount, 0/observation with unsupported characters which will be renamed to step_type, reward, discount, observation in the SavedModel.\n",
            "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7fbb020e9c10>\". Calling saved_model.distribution() will raise the following assertion error: EpsilonGreedyPolicy does not support distributions yet.\n",
            "WARNING:absl:Found untraced functions such as sequential_2_layer_call_fn, sequential_2_layer_call_and_return_conditional_losses, sequential_2_layer_call_fn, sequential_2_layer_call_and_return_conditional_losses, sequential_2_layer_call_and_return_conditional_losses while saving (showing 5 of 30). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Rubiks/policy_4/assets\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Rubiks/policy_4/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step = 62500: loss = 0.006577222608029842\n",
            "step = 63000: loss = 0.011899495497345924\n",
            "step = 63500: loss = 0.004915254656225443\n",
            "step = 64000: loss = 0.013994383625686169\n",
            "scramble = 5: step = 64000: Average Return = 0.8360000252723694\n",
            "step = 64500: loss = 0.011863237246870995\n",
            "step = 65000: loss = 0.007667264435440302\n",
            "step = 65500: loss = 0.016197752207517624\n",
            "step = 66000: loss = 0.010943952947854996\n",
            "scramble = 5: step = 66000: Average Return = 0.8759999871253967\n",
            "step = 66500: loss = 0.010921532288193703\n",
            "step = 67000: loss = 0.018347319215536118\n",
            "step = 67500: loss = 0.004422393627464771\n",
            "step = 68000: loss = 0.007972448132932186\n",
            "scramble = 5: step = 68000: Average Return = 0.8700000047683716\n",
            "step = 68500: loss = 0.015141978859901428\n",
            "step = 69000: loss = 0.020199043676257133\n",
            "step = 69500: loss = 0.018161466345191002\n",
            "step = 70000: loss = 0.010683323256671429\n",
            "scramble = 5: step = 70000: Average Return = 0.8679999709129333\n",
            "step = 70500: loss = 0.008037020452320576\n",
            "step = 71000: loss = 0.028462329879403114\n",
            "step = 71500: loss = 0.030852841213345528\n",
            "step = 72000: loss = 0.006701260805130005\n",
            "scramble = 5: step = 72000: Average Return = 0.8539999723434448\n",
            "step = 72500: loss = 0.028540216386318207\n",
            "step = 73000: loss = 0.023782027885317802\n",
            "step = 73500: loss = 0.031380921602249146\n",
            "step = 74000: loss = 0.010821012780070305\n",
            "scramble = 5: step = 74000: Average Return = 0.8920000195503235\n",
            "step = 74500: loss = 0.011584638617932796\n",
            "step = 75000: loss = 0.01236896961927414\n",
            "step = 75500: loss = 0.013310616835951805\n",
            "step = 76000: loss = 0.020027142018079758\n",
            "scramble = 5: step = 76000: Average Return = 0.8619999885559082\n",
            "step = 76500: loss = 0.008632680401206017\n",
            "step = 77000: loss = 0.004377209581434727\n",
            "step = 77500: loss = 0.014716530218720436\n",
            "step = 78000: loss = 0.0035106532741338015\n",
            "scramble = 5: step = 78000: Average Return = 0.8679999709129333\n",
            "step = 78500: loss = 0.007844504900276661\n",
            "step = 79000: loss = 0.0042737689800560474\n",
            "step = 79500: loss = 0.003931474406272173\n",
            "step = 80000: loss = 0.010031858459115028\n",
            "scramble = 5: step = 80000: Average Return = 0.878000020980835\n",
            "step = 80500: loss = 0.00992339663207531\n",
            "step = 81000: loss = 0.015878578647971153\n",
            "step = 81500: loss = 0.009560762904584408\n",
            "step = 82000: loss = 0.013509705662727356\n",
            "scramble = 5: step = 82000: Average Return = 0.8560000061988831\n",
            "step = 82500: loss = 0.012407311238348484\n",
            "step = 83000: loss = 0.01406797207891941\n",
            "step = 83500: loss = 0.015408285893499851\n",
            "step = 84000: loss = 0.011650726199150085\n",
            "scramble = 5: step = 84000: Average Return = 0.8700000047683716\n",
            "step = 84500: loss = 0.010221953503787518\n",
            "step = 85000: loss = 0.007154609076678753\n",
            "step = 85500: loss = 0.011839084327220917\n",
            "step = 86000: loss = 0.024392493069171906\n",
            "scramble = 5: step = 86000: Average Return = 0.7879999876022339\n",
            "step = 86500: loss = 0.023238740861415863\n",
            "step = 87000: loss = 0.016239222139120102\n",
            "step = 87500: loss = 0.029443491250276566\n",
            "step = 88000: loss = 0.018439114093780518\n",
            "scramble = 5: step = 88000: Average Return = 0.8840000033378601\n",
            "step = 88500: loss = 0.029614359140396118\n",
            "step = 89000: loss = 0.021589510142803192\n",
            "step = 89500: loss = 0.002916779601946473\n",
            "step = 90000: loss = 0.005410304293036461\n",
            "scramble = 5: step = 90000: Average Return = 0.8859999775886536\n",
            "step = 90500: loss = 0.0055024647153913975\n",
            "step = 91000: loss = 0.012489203363656998\n",
            "step = 91500: loss = 0.006271514110267162\n",
            "step = 92000: loss = 0.007929297164082527\n",
            "scramble = 5: step = 92000: Average Return = 0.8679999709129333\n",
            "step = 92500: loss = 0.02194570191204548\n",
            "step = 93000: loss = 0.0047971634194254875\n",
            "step = 93500: loss = 0.006125590763986111\n",
            "step = 94000: loss = 0.01130928285419941\n",
            "scramble = 5: step = 94000: Average Return = 0.8740000128746033\n",
            "step = 94500: loss = 0.013415021821856499\n",
            "step = 95000: loss = 0.00899561308324337\n",
            "step = 95500: loss = 0.005900469608604908\n",
            "step = 96000: loss = 0.009439952671527863\n",
            "scramble = 5: step = 96000: Average Return = 0.8920000195503235\n",
            "step = 96500: loss = 0.018289973959326744\n",
            "step = 97000: loss = 0.01999741420149803\n",
            "step = 97500: loss = 0.009689887054264545\n",
            "step = 98000: loss = 0.014375642873346806\n",
            "scramble = 5: step = 98000: Average Return = 0.8659999966621399\n",
            "step = 98500: loss = 0.025407319888472557\n",
            "step = 99000: loss = 0.004579433239996433\n",
            "step = 99500: loss = 0.008733568713068962\n",
            "step = 100000: loss = 0.004638501442968845\n",
            "scramble = 5: step = 100000: Average Return = 0.8999999761581421\n",
            "step = 100500: loss = 0.0148558858782053\n",
            "step = 101000: loss = 0.019296493381261826\n",
            "step = 101500: loss = 0.016843264922499657\n",
            "step = 102000: loss = 0.008990437723696232\n",
            "scramble = 5: step = 102000: Average Return = 0.9039999842643738\n",
            "step = 102500: loss = 0.010922685265541077\n",
            "step = 103000: loss = 0.014450130052864552\n",
            "step = 103500: loss = 0.008800247684121132\n",
            "step = 104000: loss = 0.009144570678472519\n",
            "scramble = 5: step = 104000: Average Return = 0.9120000004768372\n",
            "step = 104500: loss = 0.006003053858876228\n",
            "step = 105000: loss = 0.013336963020265102\n",
            "step = 105500: loss = 0.0035064329858869314\n",
            "step = 106000: loss = 0.029036276042461395\n",
            "scramble = 5: step = 106000: Average Return = 0.878000020980835\n",
            "step = 106500: loss = 0.00794576108455658\n",
            "step = 107000: loss = 0.010390110313892365\n",
            "step = 107500: loss = 0.008393545635044575\n",
            "step = 108000: loss = 0.00998023059219122\n",
            "scramble = 5: step = 108000: Average Return = 0.8920000195503235\n",
            "step = 108500: loss = 0.009906886145472527\n",
            "step = 109000: loss = 0.016978885978460312\n",
            "step = 109500: loss = 0.008398864418268204\n",
            "step = 110000: loss = 0.007212037220597267\n",
            "scramble = 5: step = 110000: Average Return = 0.8960000276565552\n",
            "step = 110500: loss = 0.016225365921854973\n",
            "step = 111000: loss = 0.010684777982532978\n",
            "step = 111500: loss = 0.020779762417078018\n",
            "step = 112000: loss = 0.005088570527732372\n",
            "scramble = 5: step = 112000: Average Return = 0.906000018119812\n",
            "step = 112500: loss = 0.015839355066418648\n",
            "step = 113000: loss = 0.01177242398262024\n",
            "step = 113500: loss = 0.00801924429833889\n",
            "step = 114000: loss = 0.0067865196615457535\n",
            "scramble = 5: step = 114000: Average Return = 0.906000018119812\n",
            "step = 114500: loss = 0.014203554019331932\n",
            "step = 115000: loss = 0.010373327881097794\n",
            "step = 115500: loss = 0.01172145176678896\n",
            "step = 116000: loss = 0.020723875612020493\n",
            "scramble = 5: step = 116000: Average Return = 0.906000018119812\n",
            "step = 116500: loss = 0.014268297702074051\n",
            "step = 117000: loss = 0.011692039668560028\n",
            "step = 117500: loss = 0.006635836325585842\n",
            "step = 118000: loss = 0.0022211181931197643\n",
            "scramble = 5: step = 118000: Average Return = 0.8999999761581421\n",
            "step = 118500: loss = 0.006062324158847332\n",
            "step = 119000: loss = 0.017958160489797592\n",
            "step = 119500: loss = 0.010275395587086678\n",
            "step = 120000: loss = 0.013031946495175362\n",
            "scramble = 5: step = 120000: Average Return = 0.9120000004768372\n",
            "step = 120500: loss = 0.028483549132943153\n",
            "step = 121000: loss = 0.025777578353881836\n",
            "step = 121500: loss = 0.024458816275000572\n",
            "step = 122000: loss = 0.010175177827477455\n",
            "scramble = 5: step = 122000: Average Return = 0.8980000019073486\n",
            "step = 122500: loss = 0.013818476349115372\n",
            "step = 123000: loss = 0.004449775442481041\n",
            "step = 123500: loss = 0.013418849557638168\n",
            "step = 124000: loss = 0.007744738832116127\n",
            "scramble = 5: step = 124000: Average Return = 0.9039999842643738\n",
            "step = 124500: loss = 0.012635331600904465\n",
            "step = 125000: loss = 0.006814469583332539\n",
            "step = 125500: loss = 0.010443254373967648\n",
            "step = 126000: loss = 0.004900457337498665\n",
            "scramble = 5: step = 126000: Average Return = 0.8920000195503235\n",
            "step = 126500: loss = 0.018188390880823135\n",
            "step = 127000: loss = 0.01687910407781601\n",
            "step = 127500: loss = 0.009502765722572803\n",
            "step = 128000: loss = 0.012548735365271568\n",
            "scramble = 5: step = 128000: Average Return = 0.8799999952316284\n",
            "step = 128500: loss = 0.021337073296308517\n",
            "step = 129000: loss = 0.009320923127233982\n",
            "step = 129500: loss = 0.00709641445428133\n",
            "step = 130000: loss = 0.002719490323215723\n",
            "scramble = 5: step = 130000: Average Return = 0.9139999747276306\n",
            "step = 130500: loss = 0.008417808450758457\n",
            "step = 131000: loss = 0.009818695485591888\n",
            "step = 131500: loss = 0.023253176361322403\n",
            "step = 132000: loss = 0.008613832294940948\n",
            "scramble = 5: step = 132000: Average Return = 0.9079999923706055\n",
            "step = 132500: loss = 0.004159645643085241\n",
            "step = 133000: loss = 0.008269527927041054\n",
            "step = 133500: loss = 0.008900146000087261\n",
            "step = 134000: loss = 0.01038304716348648\n",
            "scramble = 5: step = 134000: Average Return = 0.9179999828338623\n",
            "step = 134500: loss = 0.013106415048241615\n",
            "step = 135000: loss = 0.011586202308535576\n",
            "step = 135500: loss = 0.01063443347811699\n",
            "step = 136000: loss = 0.018039125949144363\n",
            "scramble = 5: step = 136000: Average Return = 0.906000018119812\n",
            "step = 136500: loss = 0.008669309318065643\n",
            "step = 137000: loss = 0.014557563699781895\n",
            "step = 137500: loss = 0.004587604198604822\n",
            "step = 138000: loss = 0.006460750475525856\n",
            "scramble = 5: step = 138000: Average Return = 0.8960000276565552\n",
            "step = 138500: loss = 0.005805454682558775\n",
            "step = 139000: loss = 0.0060448357835412025\n",
            "step = 139500: loss = 0.015486559830605984\n",
            "step = 140000: loss = 0.005165192298591137\n",
            "scramble = 5: step = 140000: Average Return = 0.8980000019073486\n",
            "step = 140500: loss = 0.008452543057501316\n",
            "step = 141000: loss = 0.011741952039301395\n",
            "step = 141500: loss = 0.010505113750696182\n",
            "step = 142000: loss = 0.012151197530329227\n",
            "scramble = 5: step = 142000: Average Return = 0.9160000085830688\n",
            "step = 142500: loss = 0.004614728037267923\n",
            "step = 143000: loss = 0.009374807588756084\n",
            "step = 143500: loss = 0.007951561361551285\n",
            "step = 144000: loss = 0.0030632498674094677\n",
            "scramble = 5: step = 144000: Average Return = 0.8880000114440918\n",
            "step = 144500: loss = 0.007638098206371069\n",
            "step = 145000: loss = 0.007834035903215408\n",
            "step = 145500: loss = 0.010880755260586739\n",
            "step = 146000: loss = 0.010671118274331093\n",
            "scramble = 5: step = 146000: Average Return = 0.9020000100135803\n",
            "step = 146500: loss = 0.007016777992248535\n",
            "step = 147000: loss = 0.0034423237666487694\n",
            "step = 147500: loss = 0.008260064758360386\n",
            "step = 148000: loss = 0.012702028267085552\n",
            "scramble = 5: step = 148000: Average Return = 0.8960000276565552\n",
            "step = 148500: loss = 0.010660214349627495\n",
            "step = 149000: loss = 0.007605253718793392\n",
            "step = 149500: loss = 0.011512352153658867\n",
            "step = 150000: loss = 0.010865303687751293\n",
            "scramble = 5: step = 150000: Average Return = 0.8859999775886536\n",
            "step = 150500: loss = 0.006757460068911314\n",
            "step = 151000: loss = 0.023044338449835777\n",
            "step = 151500: loss = 0.007697011344134808\n",
            "step = 152000: loss = 0.0059402212500572205\n",
            "scramble = 5: step = 152000: Average Return = 0.9259999990463257\n",
            "step = 152500: loss = 0.010565903037786484\n",
            "step = 153000: loss = 0.004308251664042473\n",
            "step = 153500: loss = 0.008880534209311008\n",
            "step = 154000: loss = 0.011736981570720673\n",
            "scramble = 5: step = 154000: Average Return = 0.8999999761581421\n",
            "step = 154500: loss = 0.012008950114250183\n",
            "step = 155000: loss = 0.007876390591263771\n",
            "step = 155500: loss = 0.007783794775605202\n",
            "step = 156000: loss = 0.004722891841083765\n",
            "scramble = 5: step = 156000: Average Return = 0.8859999775886536\n",
            "step = 156500: loss = 0.007025968283414841\n",
            "step = 157000: loss = 0.007054847199469805\n",
            "step = 157500: loss = 0.010009351186454296\n",
            "step = 158000: loss = 0.009583097882568836\n",
            "scramble = 5: step = 158000: Average Return = 0.9079999923706055\n",
            "step = 158500: loss = 0.011428899131715298\n",
            "step = 159000: loss = 0.008413434028625488\n",
            "step = 159500: loss = 0.014952275902032852\n",
            "step = 160000: loss = 0.007001389749348164\n",
            "scramble = 5: step = 160000: Average Return = 0.8960000276565552\n",
            "step = 160500: loss = 0.013096951879560947\n",
            "step = 161000: loss = 0.007256062235683203\n",
            "step = 161500: loss = 0.010753736831247807\n",
            "step = 162000: loss = 0.00956793874502182\n",
            "scramble = 5: step = 162000: Average Return = 0.9179999828338623\n",
            "step = 162500: loss = 0.007010149769484997\n",
            "step = 163000: loss = 0.005293181631714106\n",
            "step = 163500: loss = 0.003190363524481654\n",
            "step = 164000: loss = 0.003239299636334181\n",
            "scramble = 5: step = 164000: Average Return = 0.9160000085830688\n",
            "step = 164500: loss = 0.006037209182977676\n",
            "step = 165000: loss = 0.007874008268117905\n",
            "step = 165500: loss = 0.020696144551038742\n",
            "step = 166000: loss = 0.013241087086498737\n",
            "scramble = 5: step = 166000: Average Return = 0.8960000276565552\n",
            "step = 166500: loss = 0.0027180928736925125\n",
            "step = 167000: loss = 0.009172502905130386\n",
            "step = 167500: loss = 0.011271361261606216\n",
            "step = 168000: loss = 0.014655488543212414\n",
            "scramble = 5: step = 168000: Average Return = 0.9120000004768372\n",
            "step = 168500: loss = 0.010092081502079964\n",
            "step = 169000: loss = 0.004029597155749798\n",
            "step = 169500: loss = 0.016276508569717407\n",
            "step = 170000: loss = 0.0050262706354260445\n",
            "scramble = 5: step = 170000: Average Return = 0.9100000262260437\n",
            "step = 170500: loss = 0.0134555883705616\n",
            "step = 171000: loss = 0.005844904109835625\n",
            "step = 171500: loss = 0.0041479021310806274\n",
            "step = 172000: loss = 0.009622486308217049\n",
            "scramble = 5: step = 172000: Average Return = 0.8899999856948853\n",
            "step = 172500: loss = 0.014397315680980682\n",
            "step = 173000: loss = 0.012985486537218094\n",
            "step = 173500: loss = 0.007700653281062841\n",
            "step = 174000: loss = 0.006307308096438646\n",
            "scramble = 5: step = 174000: Average Return = 0.9079999923706055\n",
            "step = 174500: loss = 0.013347988948225975\n",
            "step = 175000: loss = 0.002359850099310279\n",
            "step = 175500: loss = 0.010564427822828293\n",
            "step = 176000: loss = 0.006011190824210644\n",
            "scramble = 5: step = 176000: Average Return = 0.8859999775886536\n",
            "step = 176500: loss = 0.024771153926849365\n",
            "step = 177000: loss = 0.006708554923534393\n",
            "step = 177500: loss = 0.016168365254998207\n",
            "step = 178000: loss = 0.006097264122217894\n",
            "scramble = 5: step = 178000: Average Return = 0.8880000114440918\n",
            "step = 178500: loss = 0.008129271678626537\n",
            "step = 179000: loss = 0.008081702515482903\n",
            "step = 179500: loss = 0.00972742773592472\n",
            "step = 180000: loss = 0.009396461769938469\n",
            "scramble = 5: step = 180000: Average Return = 0.8960000276565552\n",
            "step = 180500: loss = 0.008003159426152706\n",
            "step = 181000: loss = 0.00801389105618\n",
            "step = 181500: loss = 0.012742123566567898\n",
            "step = 182000: loss = 0.022369492799043655\n",
            "scramble = 5: step = 182000: Average Return = 0.9179999828338623\n",
            "step = 182500: loss = 0.013805156573653221\n",
            "step = 183000: loss = 0.012624731287360191\n",
            "step = 183500: loss = 0.0072933342307806015\n",
            "step = 184000: loss = 0.004243433941155672\n",
            "scramble = 5: step = 184000: Average Return = 0.906000018119812\n",
            "step = 184500: loss = 0.00650910846889019\n",
            "step = 185000: loss = 0.019502690061926842\n",
            "step = 185500: loss = 0.008603598922491074\n",
            "step = 186000: loss = 0.005623418837785721\n",
            "scramble = 5: step = 186000: Average Return = 0.9020000100135803\n",
            "step = 186500: loss = 0.0069488221779465675\n",
            "step = 187000: loss = 0.01702413149178028\n",
            "step = 187500: loss = 0.010934889316558838\n",
            "step = 188000: loss = 0.014510484412312508\n",
            "scramble = 5: step = 188000: Average Return = 0.8999999761581421\n",
            "step = 188500: loss = 0.010733626782894135\n",
            "step = 189000: loss = 0.0036654495634138584\n",
            "step = 189500: loss = 0.014713417738676071\n",
            "step = 190000: loss = 0.01566344127058983\n",
            "scramble = 5: step = 190000: Average Return = 0.8799999952316284\n",
            "step = 190500: loss = 0.0032866213005036116\n",
            "step = 191000: loss = 0.006072322838008404\n",
            "step = 191500: loss = 0.005250975489616394\n",
            "step = 192000: loss = 0.010153887793421745\n",
            "scramble = 5: step = 192000: Average Return = 0.8939999938011169\n",
            "step = 192500: loss = 0.026204761117696762\n",
            "step = 193000: loss = 0.006238462869077921\n",
            "step = 193500: loss = 0.014470318332314491\n",
            "step = 194000: loss = 0.004319130443036556\n",
            "scramble = 5: step = 194000: Average Return = 0.9279999732971191\n",
            "step = 194500: loss = 0.011799542233347893\n",
            "step = 195000: loss = 0.0037179230712354183\n",
            "step = 195500: loss = 0.015417663380503654\n",
            "step = 196000: loss = 0.014116955921053886\n",
            "scramble = 5: step = 196000: Average Return = 0.906000018119812\n",
            "step = 196500: loss = 0.00456221355125308\n",
            "step = 197000: loss = 0.013234224170446396\n",
            "step = 197500: loss = 0.007340800948441029\n",
            "step = 198000: loss = 0.014439592137932777\n",
            "scramble = 5: step = 198000: Average Return = 0.9079999923706055\n",
            "step = 198500: loss = 0.013385167345404625\n",
            "step = 199000: loss = 0.016881292685866356\n",
            "step = 199500: loss = 0.009686633013188839\n",
            "step = 200000: loss = 0.009223275817930698\n",
            "scramble = 5: step = 200000: Average Return = 0.9079999923706055\n",
            "step = 200500: loss = 0.004428894259035587\n",
            "step = 201000: loss = 0.00758537370711565\n",
            "step = 201500: loss = 0.013818951323628426\n",
            "step = 202000: loss = 0.008170328103005886\n",
            "scramble = 5: step = 202000: Average Return = 0.9240000247955322\n",
            "step = 202500: loss = 0.009421893395483494\n",
            "step = 203000: loss = 0.006983126513659954\n",
            "step = 203500: loss = 0.007498525083065033\n",
            "step = 204000: loss = 0.01438936498016119\n",
            "scramble = 5: step = 204000: Average Return = 0.8920000195503235\n",
            "step = 204500: loss = 0.0074419183656573296\n",
            "step = 205000: loss = 0.005133345723152161\n",
            "step = 205500: loss = 0.006509304512292147\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-ea57348e6bac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Collect a few steps and save to the replay buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tf_agents/drivers/py_driver.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m       \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m       \u001b[0mnext_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tf_agents/policies/py_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tf_agents/policies/py_tf_eager_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_nested_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# Avoid passing numpy arrays to avoid retracing of the tf.function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mpolicy_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy_action_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1484\u001b[0m   \"\"\"\n\u001b[1;32m   1485\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1486\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1494\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[1;32m    271\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 272\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attempted approaches:\n",
        "\n",
        "naive DQN\n",
        "DQN with increasing scramble number + parameter grid search\n",
        "DQN with increasing scramble number and max episode length"
      ],
      "metadata": {
        "id": "BeyrVPJWdXpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_policy = tf.compat.v2.saved_model.load(os.path.join('/content/gdrive/My Drive/Rubiks','policy_1'))"
      ],
      "metadata": {
        "id": "sDvKuU9kS75o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "07886ed32f4dc2826a1ba0878c6ca524eeafa4cf1c18d0b8c9af6981544f71a5"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "rubiks-solver.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}