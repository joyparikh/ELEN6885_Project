{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_project_TF_A2C.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Env Sanity check"
      ],
      "metadata": {
        "id": "XztHSBRQKM-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the following command to install our version of the environment\n",
        "!pip install git+https://github.com/joyparikh/ELEN6885_Project\n",
        "\n",
        "# Run the following to get baselines installed directly from the source (doing a pip install runs into some mujoco-py related error)\n",
        "#!pip install git+https://github.com/openai/baselines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd73Z2tHkHWt",
        "outputId": "89329c11-8eca-4109-d3d7-81ab120863de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/joyparikh/ELEN6885_Project\n",
            "  Cloning https://github.com/joyparikh/ELEN6885_Project to /tmp/pip-req-build-8z33r_wp\n",
            "  Running command git clone -q https://github.com/joyparikh/ELEN6885_Project /tmp/pip-req-build-8z33r_wp\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from gym-Rubiks-Cube==0.0.1) (0.17.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from gym-Rubiks-Cube==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-Rubiks-Cube==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->gym-Rubiks-Cube==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->gym-Rubiks-Cube==0.0.1) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym->gym-Rubiks-Cube==0.0.1) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-Rubiks-Cube==0.0.1) (0.16.0)\n",
            "Building wheels for collected packages: gym-Rubiks-Cube\n",
            "  Building wheel for gym-Rubiks-Cube (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-Rubiks-Cube: filename=gym_Rubiks_Cube-0.0.1-py3-none-any.whl size=6985 sha256=034bdaa113666a5f1d11b78d49d94ef0043bc0c7a9e08bac6d5655055010e356\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wu2wlpcz/wheels/ab/49/fd/daa07c1c8b5ce52abb609bea2f974b71e552a6ac20545ca66d\n",
            "Successfully built gym-Rubiks-Cube\n",
            "Installing collected packages: gym-Rubiks-Cube\n",
            "Successfully installed gym-Rubiks-Cube-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXaoil-IgBf-",
        "outputId": "9377bc87-cea4-47f7-acf5-d9b3dcd28de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 5 5 5 5 5 0 0 0 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 5 5 5 0 0 0 0 0 0 2 2 2 3\n",
            " 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4]\n",
            "         \u001b[5m\u001b[7m\u001b[32m + \u001b[0m\u001b[5m\u001b[7m\u001b[32m + \u001b[0m\u001b[5m\u001b[7m\u001b[32m + \u001b[0m                  \n",
            "         \u001b[5m\u001b[7m\u001b[32m + \u001b[0m\u001b[5m\u001b[7m\u001b[32m + \u001b[0m\u001b[5m\u001b[7m\u001b[32m + \u001b[0m                  \n",
            "         \u001b[5m\u001b[7m\u001b[32m + \u001b[0m\u001b[5m\u001b[7m\u001b[32m + \u001b[0m\u001b[5m\u001b[7m\u001b[32m + \u001b[0m                  \n",
            "\u001b[5m\u001b[7m\u001b[31m + \u001b[0m\u001b[5m\u001b[7m\u001b[31m + \u001b[0m\u001b[5m\u001b[7m\u001b[31m + \u001b[0m\u001b[5m\u001b[7m\u001b[37m + \u001b[0m\u001b[5m\u001b[7m\u001b[37m + \u001b[0m\u001b[5m\u001b[7m\u001b[37m + \u001b[0m\u001b[5m\u001b[7m\u001b[35m + \u001b[0m\u001b[5m\u001b[7m\u001b[35m + \u001b[0m\u001b[5m\u001b[7m\u001b[35m + \u001b[0m\u001b[5m\u001b[7m\u001b[33m + \u001b[0m\u001b[5m\u001b[7m\u001b[33m + \u001b[0m\u001b[5m\u001b[7m\u001b[33m + \u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[31m + \u001b[0m\u001b[5m\u001b[7m\u001b[31m + \u001b[0m\u001b[5m\u001b[7m\u001b[31m + \u001b[0m\u001b[5m\u001b[7m\u001b[37m + \u001b[0m\u001b[5m\u001b[7m\u001b[37m + \u001b[0m\u001b[5m\u001b[7m\u001b[37m + \u001b[0m\u001b[5m\u001b[7m\u001b[35m + \u001b[0m\u001b[5m\u001b[7m\u001b[35m + \u001b[0m\u001b[5m\u001b[7m\u001b[35m + \u001b[0m\u001b[5m\u001b[7m\u001b[33m + \u001b[0m\u001b[5m\u001b[7m\u001b[33m + \u001b[0m\u001b[5m\u001b[7m\u001b[33m + \u001b[0m\n",
            "\u001b[5m\u001b[7m\u001b[33m + \u001b[0m\u001b[5m\u001b[7m\u001b[33m + \u001b[0m\u001b[5m\u001b[7m\u001b[33m + \u001b[0m\u001b[5m\u001b[7m\u001b[31m + \u001b[0m\u001b[5m\u001b[7m\u001b[31m + \u001b[0m\u001b[5m\u001b[7m\u001b[31m + \u001b[0m\u001b[5m\u001b[7m\u001b[37m + \u001b[0m\u001b[5m\u001b[7m\u001b[37m + \u001b[0m\u001b[5m\u001b[7m\u001b[37m + \u001b[0m\u001b[5m\u001b[7m\u001b[35m + \u001b[0m\u001b[5m\u001b[7m\u001b[35m + \u001b[0m\u001b[5m\u001b[7m\u001b[35m + \u001b[0m\n",
            "         \u001b[5m\u001b[7m\u001b[34m + \u001b[0m\u001b[5m\u001b[7m\u001b[34m + \u001b[0m\u001b[5m\u001b[7m\u001b[34m + \u001b[0m                  \n",
            "         \u001b[5m\u001b[7m\u001b[34m + \u001b[0m\u001b[5m\u001b[7m\u001b[34m + \u001b[0m\u001b[5m\u001b[7m\u001b[34m + \u001b[0m                  \n",
            "         \u001b[5m\u001b[7m\u001b[34m + \u001b[0m\u001b[5m\u001b[7m\u001b[34m + \u001b[0m\u001b[5m\u001b[7m\u001b[34m + \u001b[0m                  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import the gym and rubiks cube environment for gym\n",
        "import gym\n",
        "import gym_Rubiks_Cube\n",
        "\n",
        "# Make the environment, replace 2x2 with 4x4. Or remove 2x2 for the normal 3x3 version\n",
        "env = gym.make(\"RubiksCube-v1\")\n",
        "\n",
        "# Checkk environment\n",
        "env.setScramble(1, 1, True)\n",
        "print(env.reset()) # print status\n",
        "env.render(mode = 'rgb_array')\n",
        "\n",
        "for i in _:\n",
        "  k = env.step(i)\n",
        "\n",
        "  if k[2]==True:\n",
        "    print(k[1]) # action 0~11\n",
        "#env.step(1) # action 0~11\n",
        "#env.render()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import useful stuff\n",
        "import collections\n",
        "import gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from typing import Any, List, Sequence, Tuple"
      ],
      "metadata": {
        "id": "Ng4CocQfALYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions for Actor-Critic Implementation"
      ],
      "metadata": {
        "id": "NADH5AfaAE1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normal Actor Critic implementation\n",
        "class ActorCritic(tf.keras.Model):\n",
        "  \"\"\"Combined actor-critic network.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self, \n",
        "      num_actions: int, \n",
        "      num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.common1 = layers.Dense(num_hidden_units, activation=\"relu\")\n",
        "    self.common2 = layers.Dense(num_hidden_units, activation=\"relu\")\n",
        "    self.common3 = layers.Dense(num_hidden_units, activation=\"relu\")\n",
        "    self.actor = layers.Dense(num_actions)\n",
        "    self.critic = layers.Dense(1)\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x1 = self.common1(inputs)\n",
        "    x2 = self.common2(x1)\n",
        "    x = self.common3(x2)\n",
        "    return self.actor(x), self.critic(x)\n",
        "\n",
        "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n",
        "# This would allow it to be included in a callable TensorFlow graph.\n",
        "\n",
        "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
        "\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  return (state.astype(np.float32), \n",
        "          np.array(reward, np.int32), \n",
        "          np.array(done, np.int32))\n",
        "\n",
        "\n",
        "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
        "  return tf.numpy_function(env_step, [action], \n",
        "                           [tf.float32, tf.int32, tf.int32])\n",
        "  \n",
        "def run_episode(\n",
        "    initial_state: tf.Tensor,  \n",
        "    model: tf.keras.Model, \n",
        "    max_steps: int) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Runs a single episode to collect training data.\"\"\"\n",
        "\n",
        "  action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "  rewards = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
        "\n",
        "  initial_state_shape = initial_state.shape\n",
        "  state = initial_state\n",
        "\n",
        "  for t in tf.range(max_steps):\n",
        "    # Convert state into a batched tensor (batch size = 1)\n",
        "    state = tf.expand_dims(state, 0)\n",
        "\n",
        "    # Run the model and to get action probabilities and critic value\n",
        "    action_logits_t, value = model(state)\n",
        "\n",
        "    # Sample next action from the action probability distribution\n",
        "    action = tf.random.categorical(action_logits_t, 1)[0, 0]\n",
        "    action_probs_t = tf.nn.softmax(action_logits_t)\n",
        "\n",
        "    # Store critic values\n",
        "    values = values.write(t, tf.squeeze(value))\n",
        "\n",
        "    # Store log probability of the action chosen\n",
        "    action_probs = action_probs.write(t, action_probs_t[0, action])\n",
        "\n",
        "    # Apply action to the environment to get next state and reward\n",
        "    state, reward, done = tf_env_step(action)\n",
        "    state.set_shape(initial_state_shape)\n",
        "\n",
        "    # Store reward\n",
        "    rewards = rewards.write(t, reward)\n",
        "\n",
        "    if tf.cast(done, tf.bool):\n",
        "      break\n",
        "\n",
        "  action_probs = action_probs.stack()\n",
        "  values = values.stack()\n",
        "  rewards = rewards.stack()\n",
        "\n",
        "  return action_probs, values, rewards\n",
        "\n",
        "def get_expected_return(\n",
        "    rewards: tf.Tensor, \n",
        "    gamma: float, \n",
        "    standardize: bool = True) -> tf.Tensor:\n",
        "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
        "\n",
        "  n = tf.shape(rewards)[0]\n",
        "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
        "\n",
        "  # Start from the end of `rewards` and accumulate reward sums\n",
        "  # into the `returns` array\n",
        "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
        "  discounted_sum = tf.constant(0.0)\n",
        "  discounted_sum_shape = discounted_sum.shape\n",
        "  for i in tf.range(n):\n",
        "    reward = rewards[i]\n",
        "    discounted_sum = reward + gamma * discounted_sum\n",
        "    discounted_sum.set_shape(discounted_sum_shape)\n",
        "    returns = returns.write(i, discounted_sum)\n",
        "  returns = returns.stack()[::-1]\n",
        "\n",
        "  if standardize:\n",
        "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
        "               (tf.math.reduce_std(returns) + eps))\n",
        "\n",
        "  return returns\n",
        "\n",
        "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "def compute_loss(\n",
        "    action_probs: tf.Tensor,  \n",
        "    values: tf.Tensor,  \n",
        "    returns: tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\"Computes the combined actor-critic loss.\"\"\"\n",
        "\n",
        "  advantage = returns - values\n",
        "\n",
        "  action_log_probs = tf.math.log(action_probs)\n",
        "  actor_loss = -tf.math.reduce_sum(action_log_probs * advantage)\n",
        "\n",
        "  critic_loss = huber_loss(values, returns)\n",
        "\n",
        "  return actor_loss + critic_loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(\n",
        "    initial_state: tf.Tensor, \n",
        "    model: tf.keras.Model, \n",
        "    optimizer: tf.keras.optimizers.Optimizer, \n",
        "    gamma: float, \n",
        "    max_steps_per_episode: int) -> tf.Tensor:\n",
        "  \"\"\"Runs a model training step.\"\"\"\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # Run the model for one episode to collect training data\n",
        "    action_probs, values, rewards = run_episode(\n",
        "        initial_state, model, max_steps_per_episode) \n",
        "\n",
        "    # Calculate expected returns\n",
        "    returns = get_expected_return(rewards, gamma)\n",
        "\n",
        "    # Convert training data to appropriate TF tensor shapes\n",
        "    action_probs, values, returns = [\n",
        "        tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
        "\n",
        "    # Calculating loss values to update our network\n",
        "    loss = compute_loss(action_probs, values, returns)\n",
        "\n",
        "  # Compute the gradients from the loss\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # Apply the gradients to the model's parameters\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  episode_reward = tf.math.reduce_sum(rewards)\n",
        "\n",
        "  return episode_reward"
      ],
      "metadata": {
        "id": "q6m-TmlDaxPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "EoEnuntTKVHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Create the environment - Experiment 1\n",
        "# Change to v1 and v2 for 2 other environments\n",
        "env = gym.make(\"RubiksCube-v0\")\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "seed = 42\n",
        "env.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Small epsilon value for stabilizing division operations\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "num_actions = env.action_space.n  # 12\n",
        "num_hidden_units = 500\n",
        "model = ActorCritic(num_actions, num_hidden_units)\n",
        "\n",
        "min_episodes_criterion = 1000\n",
        "max_episodes = 100000\n",
        "max_steps_per_episode = 100\n",
        " \n",
        "# Change for different experiments\n",
        "max_scramble_train = 1\n",
        "\n",
        "# consecutive trials\n",
        "reward_threshold = 0.93\n",
        "running_reward = 0\n",
        "\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.98\n",
        "\n",
        "# Keep last episodes reward\n",
        "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
        "\n",
        "with tqdm.trange(max_episodes) as t:\n",
        "  for i in t:\n",
        "    env.setScramble(1, max_scramble_train, True)\n",
        "    env.setStepMax(max_scramble_train*3 + 1)\n",
        "    initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "    episode_reward = int(train_step(\n",
        "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "\n",
        "    episodes_reward.append(episode_reward)\n",
        "    running_reward = statistics.mean(episodes_reward)\n",
        "\n",
        "    t.set_description(f'Episode {i}')\n",
        "    t.set_postfix(\n",
        "        episode_reward=episode_reward, running_reward=running_reward)\n",
        "\n",
        "    # Show average episode reward every 10 episodes\n",
        "    if i % 10 == 0:\n",
        "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
        "\n",
        "    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
        "        break\n",
        "\n",
        "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwpDkt45tD7j",
        "outputId": "15b40db2-485e-49b9-ae31-c2f81d3648c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 99999: 100%|██████████| 100000/100000 [35:19<00:00, 47.18it/s, episode_reward=0, running_reward=0.131]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Solved at episode 99999: average reward: 0.13!\n",
            "CPU times: user 41min 22s, sys: 2min 51s, total: 44min 13s\n",
            "Wall time: 35min 19s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "name = 'model_pkl_exp1'\n",
        "with open(name, 'wb') as obj:\n",
        "    pickle.dump(model, obj)\n",
        "files.download('/content/'+name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "EmN6iQOIlgvE",
        "outputId": "6e92f4ad-9470-4cd7-a06a-6b14cab214a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://1cb92d3b-e8f0-4984-b456-4c457163313e/assets\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_4465689d-c3e8-45e3-89c6-18e06f3d2f2d\", \"model_pkl_exp1\", 205026)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(name , 'rb') as f:\n",
        "    lr = pickle.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pFoCKAQHubZ",
        "outputId": "1a8f51d6-96e7-432a-d657-79b5b3d4a705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "KxWlYl-iCPtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Render an episode and save as a GIF file\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "#from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "#display = Display(visible=0, size=(400, 300))\n",
        "#display.start()\n",
        "\n",
        "\n",
        "env.setScramble(1, 1, True)\n",
        "print(env.reset()) # print status\n",
        "env.render()\n",
        "\n",
        "\n",
        "state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "for i in range(1, max_steps + 1):\n",
        "  state = tf.expand_dims(state, 0)\n",
        "  action_probs, _ = model(state)\n",
        "  action = np.argmax(np.squeeze(action_probs))\n",
        "\n",
        "  state, _, done, _ = env.step(action)\n",
        "  state = tf.constant(state, dtype=tf.float32)\n",
        "\n",
        "  # Render screen every 10 steps\n",
        "  if i % 10 == 0:\n",
        "    env.render(mode='rgb_array')\n",
        "\n",
        "  if done:\n",
        "    break\n"
      ],
      "metadata": {
        "id": "-kTJSp7_aUPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test scrambles"
      ],
      "metadata": {
        "id": "XEImPvvcCTQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(1,6):\n",
        "  count = 0\n",
        "  for i in range(1000):\n",
        "    env.setScramble(1, j, True)\n",
        "    state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "    for i in range(1, 99 + 1):\n",
        "      state = tf.expand_dims(state, 0)\n",
        "      action_probs, _ = lr(state)\n",
        "      action = np.argmax(np.squeeze(action_probs))\n",
        "\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      state = tf.constant(state, dtype=tf.float32)\n",
        "      \n",
        "      if done and reward ==1 :\n",
        "        count = count+reward\n",
        "        break\n",
        "\n",
        "  print(count/10,' %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTTfsLf1B4r0",
        "outputId": "0bf13d94-33ea-46fa-b18b-bcebd7bb63fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0  %\n",
            "0.0  %\n",
            "0.0  %\n",
            "0.0  %\n",
            "0.0  %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Create the environment - Experiment 1\n",
        "# Change to v1 and v2 for 2 other environments\n",
        "env = gym.make(\"ERubiksCube-v2\")\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "seed = 42\n",
        "env.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Small epsilon value for stabilizing division operations\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "num_actions = env.action_space.n  # 12\n",
        "num_hidden_units = 100\n",
        "model = ActorCritic(num_actions, num_hidden_units)\n",
        "\n",
        "min_episodes_criterion = 1000\n",
        "max_episodes = 100000\n",
        "max_steps_per_episode = 100\n",
        " \n",
        "# Change for different experiments\n",
        "max_scramble_train = 5\n",
        "\n",
        "# consecutive trials\n",
        "reward_threshold = 1\n",
        "running_reward = 0\n",
        "\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.9\n",
        "\n",
        "# Keep last episodes reward\n",
        "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
        "\n",
        "with tqdm.trange(max_episodes) as t:\n",
        "  for i in t:\n",
        "    env.setScramble(1, max_scramble_train, True)\n",
        "    initial_state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "    episode_reward = int(train_step(\n",
        "        initial_state, model, optimizer, gamma, max_steps_per_episode))\n",
        "\n",
        "    episodes_reward.append(episode_reward)\n",
        "    running_reward = statistics.mean(episodes_reward)\n",
        "\n",
        "    t.set_description(f'Episode {i}')\n",
        "    t.set_postfix(\n",
        "        episode_reward=episode_reward, running_reward=running_reward)\n",
        "\n",
        "    # Show average episode reward every 10 episodes\n",
        "    if i % 10 == 0:\n",
        "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
        "\n",
        "    if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
        "        break\n",
        "\n",
        "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9e8676-b69a-4c4c-8087-886f1ae618f5",
        "id": "JA0OIERsZY7i"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episode 99999: 100%|██████████| 100000/100000 [2:07:28<00:00, 13.07it/s, episode_reward=-100, running_reward=-89.8]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Solved at episode 99999: average reward: -89.85!\n",
            "CPU times: user 2h 50min 28s, sys: 6min 57s, total: 2h 57min 25s\n",
            "Wall time: 2h 7min 28s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "name = 'model_pkl_exp2'\n",
        "with open(name, 'wb') as obj:\n",
        "    pickle.dump(model, obj)\n",
        "files.download('/content/'+name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "7Kms9ZZnZY7l",
        "outputId": "742d25a4-3714-4f5f-a783-30b28c3fc81d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://a80dea40-426f-4274-8918-e72b7c512f59/assets\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_182498d7-3b2b-4bd8-aa76-8050a04ec741\", \"model_pkl_exp2\", 205026)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(name , 'rb') as f:\n",
        "    lr = pickle.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJl8bQd2ZY7m",
        "outputId": "a0d81d8f-00f3-4639-fa57-3ecbd301740e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    }
  ]
}